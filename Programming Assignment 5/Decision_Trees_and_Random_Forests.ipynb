{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1060632",
      "metadata": {
        "id": "c1060632"
      },
      "source": [
        "# Introduction to Decision Trees and Random Forests\n",
        "This notebook introduces two popular machine learning algorithms: Decision Trees and Random Forests. We will explore these algorithms through practical examples and visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees\n",
        "\n",
        "A **Decision Tree** is a supervised machine learning algorithm used for classification and regression. It splits the dataset into smaller subsets based on feature values, recursively forming a tree-like structure.\n",
        "\n",
        "### Advantages:\n",
        "- Easy to interpret\n",
        "- Simple to visualize\n",
        "- Handles numerical and categorical data\n",
        "\n",
        "### Disadvantages:\n",
        "- Prone to overfitting\n",
        "- Sensitive to small changes in data"
      ],
      "metadata": {
        "id": "-lBJcU855r9_"
      },
      "id": "-lBJcU855r9_"
    },
    {
      "cell_type": "markdown",
      "id": "56c6ac3c",
      "metadata": {
        "id": "56c6ac3c"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a14de38",
      "metadata": {
        "id": "8a14de38"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbda9df4",
      "metadata": {
        "id": "cbda9df4"
      },
      "source": [
        "## Load and Explore the Dataset\n",
        "We'll use the Iris dataset for this demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27953cb",
      "metadata": {
        "id": "d27953cb"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# DataFrame to view data\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['species'] = pd.Categorical.from_codes(y, iris.target_names)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45cae004",
      "metadata": {
        "id": "45cae004"
      },
      "source": [
        "## Splitting the dataset\n",
        "Split the data into training (70%) and testing (30%) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9baea7cf",
      "metadata": {
        "id": "9baea7cf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3637fb9f",
      "metadata": {
        "id": "3637fb9f"
      },
      "source": [
        "## Training a Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e21ef9c",
      "metadata": {
        "id": "2e21ef9c"
      },
      "outputs": [],
      "source": [
        "dt_clf = DecisionTreeClassifier(random_state=14)\n",
        "dt_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0b864b",
      "metadata": {
        "id": "dd0b864b"
      },
      "source": [
        "## Visualizing the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1b5679",
      "metadata": {
        "id": "6d1b5679"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plot_tree(dt_clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.title('Decision Tree Visualization')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f0e13e",
      "metadata": {
        "id": "59f0e13e"
      },
      "source": [
        "## Evaluating Decision Tree Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28923f7b",
      "metadata": {
        "id": "28923f7b"
      },
      "outputs": [],
      "source": [
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f'Decision Tree Accuracy: {accuracy_dt:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forests\n",
        "\n",
        "A **Random Forest** is an ensemble method that builds multiple decision trees, using random subsets of data and features. It aggregates predictions from these trees to provide improved accuracy and reduce overfitting.\n",
        "\n",
        "### Advantages:\n",
        "- Less prone to overfitting\n",
        "- High accuracy\n",
        "- Handles large datasets efficiently\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "34tPlCte58Ax"
      },
      "id": "34tPlCte58Ax"
    },
    {
      "cell_type": "markdown",
      "id": "434455a2",
      "metadata": {
        "id": "434455a2"
      },
      "source": [
        "## Training a Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b10384a",
      "metadata": {
        "id": "8b10384a"
      },
      "outputs": [],
      "source": [
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=14)\n",
        "rf_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing a single tree from Random Forest\n",
        "estimator = rf_clf.estimators_[0]  # choosing the first tree\n",
        "\n",
        "plt.figure(figsize=(16,10))\n",
        "# Pass the 'estimator' object directly as the 'decision_tree' argument\n",
        "plot_tree(decision_tree=estimator, # Changed 'estimator=' to 'decision_tree='\n",
        "          feature_names=iris.feature_names,\n",
        "          class_names=iris.target_names,\n",
        "          filled=True)\n",
        "plt.title('Visualization of a Single Decision Tree from Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ov5ibJnY4yqi"
      },
      "id": "Ov5ibJnY4yqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1bb9f222",
      "metadata": {
        "id": "1bb9f222"
      },
      "source": [
        "## Evaluating Random Forest Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154938e2",
      "metadata": {
        "id": "154938e2"
      },
      "outputs": [],
      "source": [
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f'Random Forest Accuracy: {accuracy_rf:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8be694",
      "metadata": {
        "id": "ae8be694"
      },
      "source": [
        "## Feature Importance from Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Importance** is a technique to measure the relative significance of each feature used by a Random Forest model. Features with higher importance have a greater impact on predicting outcomes. Identifying important features helps in feature selection and provides insights into the data.\n"
      ],
      "metadata": {
        "id": "fxTK9XT87Xr-"
      },
      "id": "fxTK9XT87Xr-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c978f6cf",
      "metadata": {
        "id": "c978f6cf"
      },
      "outputs": [],
      "source": [
        "importances = rf_clf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for idx in indices:\n",
        "    print(f\"{iris.feature_names[idx]}: {importances[idx]:.4f}\")\n",
        "\n",
        "# Visualize Feature Importance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.title(\"Feature Importances in Random Forest\")\n",
        "plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices], rotation=45)\n",
        "plt.ylabel('Importance Score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48818346",
      "metadata": {
        "id": "48818346"
      },
      "source": [
        "## Conclusion\n",
        "We explored Decision Trees and Random Forests, visualized their structures and results, and learned how to interpret feature importance. Random Forests typically outperform single Decision Trees by reducing variance and overfitting."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}